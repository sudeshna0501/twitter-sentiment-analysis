{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMemMUSvrrMIavXyjM6FSMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudeshna0501/twitter-sentiment-analysis/blob/main/LSTM_%2B_CNN_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGu5d_8Suip5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wUVqR_oGAXi",
        "outputId": "c5f8eaa7-8f82-43f0-879e-723a923019dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/gdrive/My Drive/Tweets.csv')"
      ],
      "metadata": {
        "id": "WqZahC3HGXNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= df['text']\n",
        "Y= df['airline_sentiment']"
      ],
      "metadata": {
        "id": "bcX_7CwpGdep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "4NS336qDGjGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(df['airline_sentiment'])"
      ],
      "metadata": {
        "id": "08shx7-ZGyDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\n"
      ],
      "metadata": {
        "id": "gM8dHdONG7No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWcmX2i4G94g",
        "outputId": "d0ca59e7-b9bf-4665-f1b1-a0bc4d04f7a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens"
      ],
      "metadata": {
        "id": "Qx36k4ElHBEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.apply(clean_doc)"
      ],
      "metadata": {
        "id": "8vCsTVf-HGrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Lz33KRhJH-Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = \\\n",
        "train_test_split(X, dummy_y,test_size=0.2)"
      ],
      "metadata": {
        "id": "GlX10NN4IANt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lstm = X_train"
      ],
      "metadata": {
        "id": "t8cr6aBAkb9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def joining(ele):\n",
        " return \" \".join(str(v) for v in ele)"
      ],
      "metadata": {
        "id": "Mq8ScqcvHMBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_X=X_train_lstm.apply(joining)\n"
      ],
      "metadata": {
        "id": "0F2ID73_HNs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snt_emb = X_train_lstm.apply(joining)"
      ],
      "metadata": {
        "id": "PH1DtQ0_ukcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(word):\n",
        "    return list(word)"
      ],
      "metadata": {
        "id": "RY9R-y0tHwlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_X=char_X.apply(split)"
      ],
      "metadata": {
        "id": "Ul2nPLfYHzS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_X"
      ],
      "metadata": {
        "id": "AfX-Ak7CyqyR",
        "outputId": "1ab4289a-4cfc-417c-90ac-e10e469133d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11737    [u, s, a, i, r, w, a, y, s,  , c, o, o, l,  , ...\n",
              "3311     [u, n, i, t, e, d,  , D, o, n, n, a,  , J, u, ...\n",
              "7270     [J, e, t, B, l, u, e,  , t, h, a, t, s,  , o, ...\n",
              "6791     [M, o, r, e,  , l, i, k, e,  , B, a, e, J, e, ...\n",
              "6106     [S, o, u, t, h, w, e, s, t, A, i, r,  , w, i, ...\n",
              "                               ...                        \n",
              "14272    [n, o, t, h, e, l, p, f, u, l,  , M, T,  , A, ...\n",
              "1716     [u, n, i, t, e, d,  , f, l, i, g, h, t,  , d, ...\n",
              "9271     [U, S, A, i, r, w, a, y, s,  , y, e, t,  , F, ...\n",
              "5045     [S, o, u, t, h, w, e, s, t, A, i, r,  , H, U, ...\n",
              "8841     [J, e, t, B, l, u, e,  , p, l, s,  , f, i, n, ...\n",
              "Name: text, Length: 11712, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n"
      ],
      "metadata": {
        "id": "qeaRB5j8kxIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        " tokenizer = Tokenizer()\n",
        " tokenizer.fit_on_texts(lines)\n",
        " return tokenizer\n",
        "\n",
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        " return max([len(s.split()) for s in lines])\n",
        "\n",
        "\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        " # integer encode\n",
        " encoded = tokenizer.texts_to_sequences(lines)\n",
        " # pad encoded sequences\n",
        " padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        " return padded\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#for calculating length\n",
        "def joining(ele):\n",
        " return \" \".join(str(v) for v in ele)\n"
      ],
      "metadata": {
        "id": "6ywXQ_3vlBVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for cnn\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer_cnn= create_tokenizer(X_train)\n",
        "# calculate max document length\n",
        "joinned_train_cnn=X_train.apply(joining)\n",
        "length_cnn= max_length(joinned_train_cnn)\n",
        "# calculate vocabulary size\n",
        "vocab_size_cnn = len(tokenizer_cnn.word_index) + 1\n",
        "print('Max document length: %d' % length_cnn)\n",
        "print('Vocabulary size: %d' % vocab_size_cnn)\n",
        "# encode data\n",
        "trainX_cnn= encode_text(tokenizer_cnn, X_train, length_cnn)\n",
        "print(trainX_cnn.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8P6PzqzlE48",
        "outputId": "fb3c383f-1b66-4f50-9045-62c91ee44ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 23\n",
            "Vocabulary size: 11578\n",
            "(11712, 23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for lstm\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer_lstm = create_tokenizer(char_X)\n",
        "# calculate max document length\n",
        "joinned_train_lstm =char_X.apply(joining)\n",
        "length_lstm = max_length(joinned_train_lstm)\n",
        "# calculate vocabulary size\n",
        "vocab_size_lstm = len(tokenizer_lstm.word_index) + 1\n",
        "print('Max document length: %d' % length_lstm)\n",
        "print('Vocabulary size: %d' % vocab_size_lstm)\n",
        "# encode data\n",
        "trainX_lstm = encode_text(tokenizer_lstm, char_X, length_lstm)\n",
        "print(trainX_lstm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHI-eSHBlJkj",
        "outputId": "5a745474-a62f-49c0-a6f2-51ca48e0d844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 127\n",
            "Vocabulary size: 34\n",
            "(11712, 127)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def define_model(length_lstm, length_cnn, vocab_size_lstm, vocab_size_cnn):\n",
        " # channel 1\n",
        " inputs1 = Input(shape=(length_lstm,))\n",
        " embedding1 = Embedding(vocab_size_lstm, 128, input_length=length_lstm)(inputs1)\n",
        " lstm1= (LSTM(100))(embedding1)\n",
        " flat1 =(Dropout(0.5))(lstm1)\n",
        " # channel 2\n",
        " inputs2 = Input(shape=(length_cnn,))\n",
        " embedding2 = Embedding(vocab_size_cnn, 100)(inputs2)\n",
        " conv2 = Conv1D(filters=16, kernel_size=2, activation='relu')(embedding2)\n",
        " drop2 = Dropout(0.5)(conv2)\n",
        " pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        " flat2 = Flatten()(pool2)\n",
        "\n",
        " #channel 3\n",
        " inputs3 = Input(shape=(length_cnn,))\n",
        " embedding3 = Embedding(vocab_size_cnn, 100)(inputs3)\n",
        " conv3 = Conv1D(filters=32, kernel_size=3, activation='relu')(embedding3)\n",
        " drop3 = Dropout(0.5)(conv3)\n",
        " pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        " flat3 = Flatten()(pool3)\n",
        "\n",
        "\n",
        "\n",
        "#flat1, flat2, flat3\n",
        " # merge\n",
        " merged = concatenate([flat1, flat2, flat3])\n",
        " # interpretation\n",
        " dense1 = Dense(300, activation='relu')(merged)\n",
        " dense2 = Dense(100, activation='relu')(dense1)\n",
        " dense3 = Dense(20, activation='relu')(dense2)\n",
        " dense4 = Dense(10, activation='relu')(dense3)\n",
        " outputs = Dense(3, activation='softmax')(dense4)\n",
        "\n",
        "#inputs1, inputs2,inputs3\n",
        " model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        " # compile\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " # summarize\n",
        " print(model.summary())\n",
        " plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        " return model"
      ],
      "metadata": {
        "id": "vNwxLEsOmPBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define model\n",
        "model = define_model(length_lstm, length_cnn, vocab_size_lstm,vocab_size_cnn)\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "# fit model\n",
        "model.fit([trainX_lstm, trainX_cnn, trainX_cnn], y_train, epochs=5, batch_size=20)\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU2_F3Flvfxb",
        "outputId": "b56ae8da-1968-4dc7-f1d6-71b12c5d26f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 23, 100)      1157800     ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 23, 100)      1157800     ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 127)]        0           []                               \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 22, 16)       3216        ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 21, 32)       9632        ['embedding_5[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 127, 128)     4352        ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 22, 16)       0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 21, 32)       0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 100)          91600       ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 11, 16)      0           ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 10, 32)      0           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 100)          0           ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 176)          0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 320)          0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 596)          0           ['dropout_3[0][0]',              \n",
            "                                                                  'flatten_2[0][0]',              \n",
            "                                                                  'flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 300)          179100      ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 100)          30100       ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 20)           2020        ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 10)           210         ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 3)            33          ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,635,863\n",
            "Trainable params: 2,635,863\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 23, 100)      1157800     ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 23, 100)      1157800     ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 127)]        0           []                               \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 22, 16)       3216        ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 21, 32)       9632        ['embedding_5[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 127, 128)     4352        ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 22, 16)       0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 21, 32)       0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 100)          91600       ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 11, 16)      0           ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 10, 32)      0           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 100)          0           ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 176)          0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 320)          0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 596)          0           ['dropout_3[0][0]',              \n",
            "                                                                  'flatten_2[0][0]',              \n",
            "                                                                  'flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 300)          179100      ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 100)          30100       ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 20)           2020        ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 10)           210         ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 3)            33          ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,635,863\n",
            "Trainable params: 2,635,863\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "586/586 [==============================] - 27s 16ms/step - loss: 0.6713 - accuracy: 0.7135\n",
            "Epoch 2/5\n",
            "586/586 [==============================] - 9s 16ms/step - loss: 0.4217 - accuracy: 0.8334\n",
            "Epoch 3/5\n",
            "586/586 [==============================] - 8s 14ms/step - loss: 0.2606 - accuracy: 0.9076\n",
            "Epoch 4/5\n",
            "586/586 [==============================] - 9s 16ms/step - loss: 0.1768 - accuracy: 0.9374\n",
            "Epoch 5/5\n",
            "586/586 [==============================] - 7s 11ms/step - loss: 0.1298 - accuracy: 0.9541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_lstm=X_test"
      ],
      "metadata": {
        "id": "NSIcDO4hm4wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def joining(ele):\n",
        " return \" \".join(str(v) for v in ele)"
      ],
      "metadata": {
        "id": "fv11mCB1nB4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_X_test=X_test_lstm.apply(joining)"
      ],
      "metadata": {
        "id": "MDNe2sCInLKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(word):\n",
        "    return list(word)"
      ],
      "metadata": {
        "id": "r3mV4zJznNIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_X_test=char_X_test.apply(split)"
      ],
      "metadata": {
        "id": "_efqTYSPnWyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_vectors_test = X_test_lstm.apply(joining)"
      ],
      "metadata": {
        "id": "nfPG7-zxcIth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        "\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "\t# integer encode\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad encoded sequences\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\treturn padded\n",
        "\n",
        "  #for calculating length\n",
        "def joining(ele):\n",
        "  return \" \".join(str(v) for v in ele)\n",
        "\n",
        "\n",
        "#for cnn\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer_cnn= create_tokenizer(X_train)\n",
        "# calculate max document length\n",
        "joinned_train_cnn=X_train.apply(joining)\n",
        "length_cnn= max_length(joinned_train_cnn)\n",
        "# calculate vocabulary size\n",
        "vocab_size_cnn = len(tokenizer_cnn.word_index) + 1\n",
        "print('Max document length: %d' % length_cnn)\n",
        "print('Vocabulary size: %d' % vocab_size_cnn)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer_cnn, X_train, length_cnn)\n",
        "testX = encode_text(tokenizer_cnn, X_test, length_cnn)\n",
        "print(trainX.shape)\n",
        "\n",
        "#for lstm\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer_lstm = create_tokenizer(char_X)\n",
        "# calculate max document length\n",
        "joinned_train_lstm =char_X.apply(joining)\n",
        "length_lstm = max_length(joinned_train_lstm)\n",
        "# calculate vocabulary size\n",
        "vocab_size_lstm = len(tokenizer_lstm.word_index) + 1\n",
        "print('Max document length: %d' % length_lstm)\n",
        "print('Vocabulary size: %d' % vocab_size_lstm)\n",
        "# encode data\n",
        "trainX_lstm = encode_text(tokenizer_lstm, char_X, length_lstm)\n",
        "testX_lstm= encode_text(tokenizer_lstm, char_X_test, length_lstm)\n",
        "print(trainX_lstm.shape)\n",
        "\n",
        "\n",
        "\n",
        "#testLabels = list(test_data['sentiment'])\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# evaluate model on training dataset\n",
        "loss, acc = model.evaluate([trainX_lstm, trainX, trainX], y_train, verbose=0)\n",
        "print('Train Accuracy: %f' % (acc*100))\n",
        "\n",
        "# evaluate model on test dataset dataset\n",
        "loss, acc = model.evaluate([testX_lstm, testX, testX], y_test, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "id": "4Xs1pUwklXUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbbb957-13b8-4814-9cdf-724c74486572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 23\n",
            "Vocabulary size: 11578\n",
            "(11712, 23)\n",
            "Max document length: 127\n",
            "Vocabulary size: 34\n",
            "(11712, 127)\n",
            "Train Accuracy: 98.326504\n",
            "Test Accuracy: 76.707649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xw50wKJjCZf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}