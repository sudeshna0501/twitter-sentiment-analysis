{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/cjEFZiyM/iabYQrhhiYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudeshna0501/twitter-sentiment-analysis/blob/main/Multichannel_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAWfqCSngXDk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGpAtaOs6O0r",
        "outputId": "ec25e22e-497f-4731-d71d-5548d77b5510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/gdrive/My Drive/Tweets.csv')"
      ],
      "metadata": {
        "id": "svfk91nY6Uc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= df['text']\n",
        "Y= df['airline_sentiment']"
      ],
      "metadata": {
        "id": "mO6HPlZRXAsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "UvaihcHZB8PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RI9_ISUT-Q6",
        "outputId": "98ae23e9-7e9f-4e9b-ad4e-402734884d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         neutral\n",
              "1        positive\n",
              "2         neutral\n",
              "3        negative\n",
              "4        negative\n",
              "           ...   \n",
              "14635    positive\n",
              "14636    negative\n",
              "14637     neutral\n",
              "14638    negative\n",
              "14639     neutral\n",
              "Name: airline_sentiment, Length: 14640, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)"
      ],
      "metadata": {
        "id": "T5Z4_QtDJ_wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c31fd1-b7c7-4946-934c-3cf5a5bd240f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "k7SfD95jH1WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_Y = encoder.transform(df['airline_sentiment'])\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)"
      ],
      "metadata": {
        "id": "tc-x17dqHqrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhsNPNRWjGY7",
        "outputId": "8853b0b6-9d6e-458a-916b-4d651b100c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "K75V-iRRKLL4",
        "outputId": "5ee4bdf7-0ff7-4a97-e7d3-7b701309b55f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "0  570306133677760513           neutral                        1.0000   \n",
              "1  570301130888122368          positive                        0.3486   \n",
              "2  570301083672813571           neutral                        0.6837   \n",
              "3  570301031407624196          negative                        1.0000   \n",
              "4  570300817074462722          negative                        1.0000   \n",
              "\n",
              "  negativereason  negativereason_confidence         airline  \\\n",
              "0            NaN                        NaN  Virgin America   \n",
              "1            NaN                     0.0000  Virgin America   \n",
              "2            NaN                        NaN  Virgin America   \n",
              "3     Bad Flight                     0.7033  Virgin America   \n",
              "4     Can't Tell                     1.0000  Virgin America   \n",
              "\n",
              "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
              "0                    NaN     cairdin                 NaN              0   \n",
              "1                    NaN    jnardino                 NaN              0   \n",
              "2                    NaN  yvonnalynn                 NaN              0   \n",
              "3                    NaN    jnardino                 NaN              0   \n",
              "4                    NaN    jnardino                 NaN              0   \n",
              "\n",
              "                                                text tweet_coord  \\\n",
              "0                @VirginAmerica What @dhepburn said.         NaN   \n",
              "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
              "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
              "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
              "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
              "\n",
              "               tweet_created tweet_location               user_timezone  \n",
              "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
              "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
              "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
              "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
              "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71915a6b-22c6-44c8-b770-6b944d38577e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71915a6b-22c6-44c8-b770-6b944d38577e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71915a6b-22c6-44c8-b770-6b944d38577e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71915a6b-22c6-44c8-b770-6b944d38577e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "mEcbhpDvCJ2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTcwl6fXBx-d",
        "outputId": "afa743df-6cfe-40a9-b2a4-81ad0d19b23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens"
      ],
      "metadata": {
        "id": "q2fNK2yxBJmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.apply(clean_doc)"
      ],
      "metadata": {
        "id": "4UBTAsUCBOVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OJiDREvCQTW",
        "outputId": "dea65ef0-ad87-4770-cdba-3c7bdf251c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                    [VirginAmerica, What, dhepburn, said]\n",
              "1        [VirginAmerica, plus, youve, added, commercial...\n",
              "2        [VirginAmerica, didnt, today, Must, mean, need...\n",
              "3        [VirginAmerica, really, aggressive, blast, obn...\n",
              "4                 [VirginAmerica, really, big, bad, thing]\n",
              "                               ...                        \n",
              "14635    [AmericanAir, thank, got, different, flight, C...\n",
              "14636    [AmericanAir, leaving, minutes, Late, Flight, ...\n",
              "14637     [AmericanAir, Please, bring, American, Airlines]\n",
              "14638    [AmericanAir, money, change, flight, dont, ans...\n",
              "14639    [AmericanAir, ppl, need, know, many, seats, ne...\n",
              "Name: text, Length: 14640, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "Uer4FdCZCy19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = \\\n",
        "train_test_split(X, dummy_y,test_size=0.2)"
      ],
      "metadata": {
        "id": "a05KlJoRC3-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp0MgKzgDDeT",
        "outputId": "6c09620f-1ff0-4d15-f192-ac6f2dba9e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2928, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        " tokenizer = Tokenizer()\n",
        " tokenizer.fit_on_texts(lines)\n",
        " return tokenizer\n",
        "\n",
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        " return max([len(s.split()) for s in lines])\n",
        "\n",
        "\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        " # integer encode\n",
        " encoded = tokenizer.texts_to_sequences(lines)\n",
        " # pad encoded sequences\n",
        " padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        " return padded\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#for calculating length\n",
        "def joining(ele):\n",
        " return \" \".join(str(v) for v in ele)\n",
        "\n",
        "\n",
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        " # channel 1\n",
        " inputs1 = Input(shape=(length,))\n",
        " embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        " conv1 = Conv1D(filters=32, kernel_size=1, activation='relu')(embedding1)\n",
        " drop1 = Dropout(0.5)(conv1)\n",
        " pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        " flat1 = Flatten()(pool1)\n",
        " # channel 2\n",
        " inputs2 = Input(shape=(length,))\n",
        " embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        " conv2 = Conv1D(filters=32, kernel_size=2, activation='relu')(embedding2)\n",
        " drop2 = Dropout(0.5)(conv2)\n",
        " pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        " flat2 = Flatten()(pool2)\n",
        " # channel 3\n",
        " inputs3 = Input(shape=(length,))\n",
        " embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        " conv3 = Conv1D(filters=32, kernel_size=3, activation='relu')(embedding3)\n",
        " drop3 = Dropout(0.5)(conv3)\n",
        " pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        " flat3 = Flatten()(pool3)\n",
        " # merge\n",
        " merged = concatenate([flat1, flat2, flat3])\n",
        " # interpretation\n",
        " dense1 = Dense(4000, activation='relu')(merged)\n",
        " dense2 = Dense(2000, activation='relu')(dense1)\n",
        " dense3 = Dense(1000, activation='relu')(dense2)\n",
        " dense4 = Dense(500, activation='relu')(dense3)\n",
        " dense5 = Dense(200, activation='relu')(dense4)\n",
        " dense6 = Dense(100, activation='relu')(dense5)\n",
        " dense7 = Dense(20, activation='relu')(dense6)\n",
        " outputs = Dense(3, activation='softmax')(dense7)\n",
        " model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        " # compile\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " # summarize\n",
        " print(model.summary())\n",
        " plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        " return model\n",
        "\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(X_train)\n",
        "# calculate max document length\n",
        "joinned_train =X_train.apply(joining)\n",
        "length = max_length(joinned_train)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, X_train, length)\n",
        "print(trainX.shape)\n",
        "\n",
        "\n",
        "# define model\n",
        "model = define_model(length, vocab_size)\n",
        "print(model.summary())\n",
        "\n",
        "# fit model\n",
        "model.fit([trainX,trainX,trainX], y_train, epochs=10, batch_size=20)\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mekKuWS_S386",
        "outputId": "521c1780-8bdb-4c91-ac01-5f26fe5940b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 23\n",
            "Vocabulary size: 11562\n",
            "(11712, 23)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 23, 100)      1156200     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 23, 100)      1156200     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 23, 100)      1156200     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 23, 32)       3232        ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 22, 32)       6432        ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 21, 32)       9632        ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 23, 32)       0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 22, 32)       0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 21, 32)       0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 11, 32)       0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 11, 32)      0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 10, 32)      0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 352)          0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 352)          0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 320)          0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 1024)         0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 4000)         4100000     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2000)         8002000     ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1000)         2001000     ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 500)          500500      ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 200)          100200      ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 100)          20100       ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 20)           2020        ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 3)            63          ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 18,213,779\n",
            "Trainable params: 18,213,779\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 23)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 23, 100)      1156200     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 23, 100)      1156200     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 23, 100)      1156200     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 23, 32)       3232        ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 22, 32)       6432        ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 21, 32)       9632        ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 23, 32)       0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 22, 32)       0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 21, 32)       0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 11, 32)       0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 11, 32)      0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 10, 32)      0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 352)          0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 352)          0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 320)          0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 1024)         0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 4000)         4100000     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2000)         8002000     ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1000)         2001000     ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 500)          500500      ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 200)          100200      ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 100)          20100       ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 20)           2020        ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 3)            63          ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 18,213,779\n",
            "Trainable params: 18,213,779\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "586/586 [==============================] - 118s 199ms/step - loss: 0.6901 - accuracy: 0.7118\n",
            "Epoch 2/10\n",
            "586/586 [==============================] - 111s 189ms/step - loss: 0.4678 - accuracy: 0.8209\n",
            "Epoch 3/10\n",
            "586/586 [==============================] - 112s 191ms/step - loss: 0.3156 - accuracy: 0.8886\n",
            "Epoch 4/10\n",
            "586/586 [==============================] - 111s 189ms/step - loss: 0.2093 - accuracy: 0.9280\n",
            "Epoch 5/10\n",
            "586/586 [==============================] - 112s 191ms/step - loss: 0.1662 - accuracy: 0.9454\n",
            "Epoch 6/10\n",
            "586/586 [==============================] - 112s 192ms/step - loss: 0.1416 - accuracy: 0.9541\n",
            "Epoch 7/10\n",
            "586/586 [==============================] - 112s 191ms/step - loss: 0.1185 - accuracy: 0.9637\n",
            "Epoch 8/10\n",
            "586/586 [==============================] - 114s 195ms/step - loss: 0.0939 - accuracy: 0.9699\n",
            "Epoch 9/10\n",
            "586/586 [==============================] - 112s 190ms/step - loss: 0.0797 - accuracy: 0.9740\n",
            "Epoch 10/10\n",
            "586/586 [==============================] - 112s 191ms/step - loss: 0.0830 - accuracy: 0.9728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        "\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "\t# integer encode\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad encoded sequences\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\treturn padded\n",
        "\n",
        "  #for calculating length\n",
        "def joining(ele):\n",
        "  return \" \".join(str(v) for v in ele)\n",
        "\n",
        "\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(X_train)\n",
        "# calculate max document length\n",
        "# calculate max document length\n",
        "joinned_train = X_train.apply(joining)\n",
        "length = max_length(joinned_train)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer,X_train, length)\n",
        "testX = encode_text(tokenizer, X_test, length)\n",
        "print(trainX.shape, testX.shape)\n",
        "\n",
        "#testLabels = list(test_data['sentiment'])\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# evaluate model on training dataset\n",
        "loss, acc = model.evaluate([trainX,trainX,trainX], y_train, verbose=0)\n",
        "print('Train Accuracy: %f' % (acc*100))\n",
        "\n",
        "# evaluate model on test dataset dataset\n",
        "loss, acc = model.evaluate([testX,testX,testX], y_test, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "id": "b3nnleRgsDrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fde25e-5cbc-4e23-a35c-80cea99f458e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 23\n",
            "Vocabulary size: 11562\n",
            "(11712, 23) (2928, 23)\n",
            "Train Accuracy: 98.531419\n",
            "Test Accuracy: 76.229507\n"
          ]
        }
      ]
    }
  ]
}